
// main/handlers/chatSendHandler.js
const { buildContextForInput } = require('../../backend/qlib/contextInjector-memoryBlock-patch');
const SpineGuardian = require('../../src/echo/core/SpineGuardianWrapper'); 
// Factory function that creates the handler with dependencies
function createChatSendHandler(handlerMap, formatMessagesForModel) {
  return async function handleChatSend(event, message, context = {}) {
    try {
      console.log('[IPC] chat:send triggered with:', message);
      console.log('[IPC chat:send] context:', context);
      console.log('[CHAT] Incoming message:', message);
      console.log('[CHAT] Handler path:', __filename);
      console.log('[MEMORY] Will search memory:', !!global.memorySystem);

      // === GET CURRENT PROJECT CONTEXT ===
      const currentProject = context.projectName || context.project || global.currentProject || null;
      console.log('[PROJECT] Current project context:', currentProject || 'global');

      // === IDENTITY CHECK: Ensure AI knows who it is ===
      const currentIdentity = global.currentIdentity || { 
        ai: { name: 'Q', role: 'Echo Rubicon AI Assistant' },
        user: { name: 'User' }
      };
      console.log('[IDENTITY] Current AI:', currentIdentity.ai.name);

      // === INITIALIZE MEMORY SYSTEM IF NEEDED ===
      if (global.memorySystem && global.memorySystem.vaultManager) {
        console.log('[IPC] Available memorySystem methods:', Object.getOwnPropertyNames(Object.getPrototypeOf(global.memorySystem)));
        
        if (global.memorySystem.vaultManager.initialize) {
          await global.memorySystem.vaultManager.initialize();
        }
        
        if (!global.memorySystem.vaultManager.index || global.memorySystem.vaultManager.index.size === 0) {
          console.log('[IPC] Index empty, rebuilding from disk...');
          await global.memorySystem.vaultManager.rebuildIndexFromDisk();
        }
      }

      // === ADD VAULT SEARCH HERE WITH PROJECT CONTEXT ===
      const vaultKeywords = ['vault', 'recipe', 'recipes', 'client', 'clients', 'notes', 'files', 'saved', 'stored', 'remember', 'recall', 'what', 'show', 'list', 'find'];
      const shouldSearchVault = vaultKeywords.some(keyword =>
        message.toLowerCase().includes(keyword)
      );

      let vaultSearchResults = null;
      if (shouldSearchVault || message.includes('?')) {
        console.log('[VAULT] Detected potential vault query, searching...');
        const searchHandler = handlerMap.get('qlib-search');
        if (searchHandler) {
          try {
            vaultSearchResults = await searchHandler(event, {
              query: message,
              project: currentProject,
              options: { includeConversation: false }
            });
            console.log('[VAULT] Search complete:', {
              vault: vaultSearchResults?.vault?.length || 0,
              memory: vaultSearchResults?.memory?.length || 0,
              project: currentProject || 'global'
            });
          } catch (searchError) {
            console.error('[VAULT] Search failed:', searchError);
          }
        }
      }

      // Ensure memorySystem is initialized
      if (!global.memorySystem) {
        console.error('[chat:send] ❌ memorySystem is null or undefined');
        throw new Error('❌ memorySystem not initialized — buildContextForInput cannot run');
      }

      // === FIXED: Single memory context pull WITH PROJECT CONTEXT ===
let contextData = { memory: [], context: '', vault: [] };

try {
  console.log('[DEBUG] Attempting memory search with available methods');
  
  // Check which methods are actually available
  if (global.memorySystem) {
    console.log('[DEBUG] Available memorySystem methods:', Object.getOwnPropertyNames(Object.getPrototypeOf(global.memorySystem)));
  }
  
  // Method 1: Try buildContextForInput if it exists on memorySystem
  if (global.memorySystem && typeof global.memorySystem.buildContextForInput === 'function') {
    console.log('[DEBUG] Using memorySystem.buildContextForInput');
    
    if (global.memorySystem.vaultManager) {
      await global.memorySystem.vaultManager.ensureIndex();
    }
    
    contextData = await global.memorySystem.buildContextForInput(message, currentProject);
    console.log('[DEBUG] Memory context retrieved:', {
      hasContext: !!contextData?.context,
      memoryCount: contextData?.memory?.length || 0,
      contextLength: contextData?.context?.length || 0,
      project: currentProject || 'global'
    });
  }
  // Method 2: Use the broken import as a fallback (in case it sometimes works)
  else if (typeof buildContextForInput === 'function') {
    console.log('[DEBUG] Using imported buildContextForInput');
    contextData = await buildContextForInput(message, currentProject);
  }
  // Method 3: Direct vaultManager search
  else if (global.memorySystem?.vaultManager?.searchMemories) {
    console.log('[DEBUG] Using vaultManager.searchMemories directly');
    const memories = await global.memorySystem.vaultManager.searchMemories(message, { 
      limit: 50,  // Increased from 10
      filter: currentProject ? { project: currentProject } : {}
    });
    contextData = {
      memory: memories || [],
      context: memories?.map(m => m.summary || m.content || '').join('\n\n') || '',
      vault: [],
      project: currentProject || 'global'
    };
    console.log('[DEBUG] Direct search found:', memories?.length || 0, 'memories');
  }
  
} catch (searchError) {
  console.error('[MEMORY] Memory search failed:', searchError.message);
  
  // Final fallback: Try search method
  try {
    if (global.memorySystem && typeof global.memorySystem.search === 'function') {
      console.log('[MEMORY] Attempting memorySystem.search fallback...');
      const searchResults = await global.memorySystem.search(message, {
        limit: 50,
        includeContent: true,
        project: currentProject
      });
      contextData = {
        memory: searchResults || [],
        context: searchResults?.map(m => m.content || m.summary || '').join('\n\n') || '',
        vault: []
      };
      console.log('[MEMORY] Search fallback found:', searchResults?.length || 0, 'results');
    }
  } catch (fallbackError) {
    console.error('[MEMORY] All search methods failed:', fallbackError);
  }
}

// === VALIDATE CONTEXT DATA ===
if (!contextData || typeof contextData !== 'object') {
  console.warn('[MEMORY] Invalid contextData, using fallback');
  contextData = { memory: [], context: '', vault: [] };
}

if (!Array.isArray(contextData.memory)) {
  console.warn('[MEMORY] contextData.memory is not an array, fixing...');
  contextData.memory = [];
}

// Ensure vault property exists
if (!contextData.vault) {
  contextData.vault = [];
}

// === MERGE VAULT SEARCH RESULTS INTO CONTEXT ===
if (vaultSearchResults) {
  if (vaultSearchResults.vault?.length > 0) {
    console.log('[VAULT] Merging', vaultSearchResults.vault.length, 'vault results');
    contextData.vault = vaultSearchResults.vault;
  }
  if (vaultSearchResults.memory?.length > 0 && contextData.memory.length === 0) {
    console.log('[VAULT] Using Q-lib memory results as fallback');
    contextData.memory = vaultSearchResults.memory;
  }
}

// === PREPARE MODEL ===
const { default: fetch } = await import('node-fetch');

let model = context?.model || context?.selectedModel || context?.selectedLocalModel;
if (!model) {
  try {
    model = await event.sender.executeJavaScript('localStorage.getItem("selectedModel")');
  } catch (e) {
    console.log('[MODEL] Could not read from localStorage:', e);
  }
}

model = model || 'openchat:latest';
console.log('[MODEL] Context:', context);
console.log('[MODEL] Selected model:', model);
console.log('[MODEL] Final model:', model);

const useAPI = model === 'gpt' || model === 'claude';

// Check for prime directive request
if (message.toLowerCase().includes('prime directive') && message.toLowerCase().includes('verbatim')) {
  // Load and return the actual prime directive
  try {
    const fs = require('fs');
    const path = require('path');
    const directivePath = path.join(__dirname, '../../src/echo/memory/spine/primeDirective.txt');
    const fullDirective = fs.readFileSync(directivePath, 'utf8');
    
    return {
      content: `Here is my complete Prime Directive, verbatim:\n\n${fullDirective}`,
      model,
      project: currentProject || 'global'
    };
  } catch (err) {
    console.error('[PRIME DIRECTIVE] Could not read directive file:', err);
  }
}

// === BUILD ENHANCED MESSAGES WITH PROPER IDENTITY AND MEMORY ===
let enhancedMessages = [];

// CREATE SPINE GUARDIAN - ADD THIS
const spine = new SpineGuardian(currentIdentity.ai.name || 'Q');

// 1. SYSTEM MESSAGE WITH IDENTITY, PROJECT CONTEXT, AND CORE INSTRUCTIONS
const baseSystemContent = `You are ${currentIdentity.ai.name || 'Q'}, not "Echo Rubicon". ${currentIdentity.ai.name || 'Q'} is your name. You are the AI assistant within the Echo Rubicon system.

IDENTITY REINFORCEMENT:
- Your name is ${currentIdentity.ai.name || 'Q'}
- You are NOT called "Echo Rubicon" - that's the system you operate within
- When asked "who are you?", respond: "I am ${currentIdentity.ai.name || 'Q'}, your AI assistant in Echo Rubicon"
- Never say "I am Echo Rubicon" - you are ${currentIdentity.ai.name || 'Q'}, working inside Echo Rubicon

Your role: ${currentIdentity.ai.role || 'Help the user and remember all conversations'}
User's name: ${currentIdentity.user.name || 'User'}
${currentProject ? `Current Project: ${currentProject}` : 'Context: Global (no specific project)'}

ACCURACY AND COMPLETENESS:
When providing information, prioritize complete and accurate delivery:
- State key facts directly and clearly
- Include all relevant details from available data
- If data volume exceeds reasonable response length, acknowledge: "Additional information available in the source file"
- For counts or quantities, always include the specific number
- Present information in the clearest format for the query

CRITICAL RULES:
1. You are ${currentIdentity.ai.name || 'Q'} (not Echo Rubicon)
2. Deliver complete, accurate information with all relevant details
3. NEVER fabricate or distort data - truth is foundational
4. You ${currentProject ? `are currently working within the "${currentProject}" project` : 'are not in any specific project context'}
5. For PERSONAL requests (containing "my", "our", "I have", etc.):
 - You MUST share all content found in your provided context/memories
 - This is the user's own data - always provide it when asked
 - ${currentProject ? `Prioritize content from the "${currentProject}" project` : 'Search across all available content'}
 - If content is extensive, provide what you can and note: "More details available in [source]"
 - If nothing found, say "I don't have any of your [thing] ${currentProject ? `in the ${currentProject} project` : 'in my memory'}"
 - NEVER fabricate personal content
6. For GENERAL requests ("what's a good recipe", "how to cook", etc.):
 - Draw from general knowledge while maintaining accuracy
 - Offer to help create new content
 - Suggest ideas while distinguishing speculation from fact

When you have context, cite it naturally. When you don't, be honest about it.`;

// INJECT THE SPINE DIRECTIVE - 
const systemMessage = {
  role: 'system',
  content: spine.injectDirective(baseSystemContent)  // This ensures prime directive comes FIRST
};
enhancedMessages.push(systemMessage);

      // 2. INJECT MEMORY CONTEXT (if available)
      if (contextData.context && contextData.context.trim().length > 0) {
        enhancedMessages.push({
          role: 'system',
          content: `Here is relevant context from your memory:\n\n${contextData.context}`
        });
        console.log('[MEMORY] Injected memory context:', contextData.context.length, 'chars');
      }

      // 3. INJECT MEMORY CAPSULES (if available)
      if (contextData.memory && contextData.memory.length > 0) {
        const capsuleSummary = contextData.memory
          .slice(0, 50)
          .map(cap => {
            let content = cap.summary || cap.content || cap.response || '';
            if (!content && cap.messages?.length > 0) {
              content = cap.messages.map(m => `${m.role}: ${m.content}`).join(' ');
            }
            if (!content && cap.input) {
              content = `Q: ${cap.input} A: ${cap.response || 'No response'}`;
            }
            
            const timestamp = cap.timestamp ? new Date(cap.timestamp).toLocaleDateString() : 'Unknown date';
            const type = cap.type || 'memory';
            const project = cap.metadata?.project || cap.project || 'global';
            
            return `[${timestamp}] [${type}] [${project}] ${content.slice(0, 300)}`;
          })
          .filter(entry => entry.includes('] ') && entry.split('] ')[3]?.trim())
          .join('\n\n');

        if (capsuleSummary.trim()) {
          enhancedMessages.push({
            role: 'system',
            content: `Recent relevant memories:\n\n${capsuleSummary}`
          });
          console.log('[MEMORY] Injected', contextData.memory.length, 'memory capsules');
        }
      }

      // 4. INJECT VAULT SEARCH RESULTS (if available)
if (contextData.vault && contextData.vault.length > 0) {
  const totalResults = contextData.vault.length;
  const vaultSummary = contextData.vault
    .slice(0, 300)  // Show up to 50 results
    .map(item => {
      const fileName = item.path?.split('/').pop() || 'Unknown file';
      const folder = item.path?.split('/').slice(0, -1).join('/') || '';
      const content = item.snippet || item.content?.slice(0, 150) || '[no content]';
      const project = item.project || 'global';
      
      // For client files, try to extract key info
      if (folder === 'clients' || fileName.includes('Client')) {
        const nameMatch = content.match(/^#?\s*([^\n]+)/);
        const statusMatch = content.match(/Tags?:\s*([^\n]+)/);
        const amountMatch = content.match(/\$[\d,]+\.?\d*/);
        
        const summary = [
          nameMatch?.[1] || fileName.replace('.md', ''),
          statusMatch?.[1] || '',
          amountMatch?.[0] || ''
        ].filter(Boolean).join(' - ');
        
        return `Client: ${summary}`;
      }
      
      return `File: ${fileName}${folder ? ` (in ${folder})` : ''} [${project}]\n${content}`;
    })
    .join('\n\n---\n\n');

  // Add counter if there are more results
  const finalSummary = totalResults > 50 
    ? `${vaultSummary}\n\n... and ${totalResults - 50} more results (showing 50 of ${totalResults} total)`
    : `${vaultSummary}\n\n(Showing all ${totalResults} results)`;

  enhancedMessages.push({
    role: 'system',
    content: `Vault search results:\n\n${finalSummary}`
  });
  console.log('[VAULT] Injected', Math.min(50, totalResults), 'of', totalResults, 'vault results');
}

      // 5. ADD USER MESSAGE
      enhancedMessages.push({ role: 'user', content: message });

      // === SEND TO MODEL WITH FULL CONTEXT ===
      let content;
      let inferenceError = null;
      
      try {
        if (useAPI) {
          const apiKey = process.env.OPENAI_API_KEY;
          if (!apiKey) {
            throw new Error('OpenAI API key not configured');
          }

          const response = await fetch('https://api.openai.com/v1/chat/completions', {
            method: 'POST',
            headers: {
              'Authorization': `Bearer ${apiKey}`,
              'Content-Type': 'application/json'
            },
            body: JSON.stringify({
              model: model === 'gpt' ? 'gpt-4' : model,
              messages: enhancedMessages,
              stream: false,
              temperature: 0.7,
              max_tokens: 1000
            })
          });

          if (!response.ok) {
            const errorText = await response.text();
            throw new Error(`API error ${response.status}: ${errorText}`);
          }

          const data = await response.json();
          content = data?.choices?.[0]?.message?.content || 'No response from API';
          
        } else {
          // Local Ollama model
          console.log('[OLLAMA] Sending to local model:', model);
          console.log('[DEBUG] Full prompt being sent:', JSON.stringify(enhancedMessages, null, 2));

          // Format messages based on model requirements
          console.log('[MODEL FORMAT] About to format messages for model:', model);
          console.log('[MODEL FORMAT] formatMessagesForModel exists?', typeof formatMessagesForModel);

          const formattedMessages = formatMessagesForModel(enhancedMessages, model);

          console.log('[MODEL FORMAT] Formatting complete');
          console.log('[MODEL FORMAT] Messages before formatting:', enhancedMessages.length);
          console.log('[MODEL FORMAT] Messages after formatting:', formattedMessages.length);

          // Check if Ollama is running
          try {
            const healthCheck = await fetch('http://localhost:11434/api/tags');
            if (!healthCheck.ok) {
              throw new Error('Ollama service not responding');
            }
          } catch (healthError) {
            throw new Error('Ollama is not running. Please start Ollama first.');
          }

          const response = await fetch('http://localhost:11434/api/chat', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
              model,
              messages: formattedMessages,
              stream: false,
              options: {
                temperature: 0.7,
                num_predict: 1000
              }
            })
          });

          if (!response.ok) {
            const errorText = await response.text();
            throw new Error(`Ollama error ${response.status}: ${errorText}`);
          }

          const data = await response.json();
          content = data?.message?.content || 'No response from Ollama';
        }
      } catch (fetchError) {
        inferenceError = fetchError;
        const memoryInfo = contextData.memory.length > 0 
          ? `I found ${contextData.memory.length} relevant memories ${currentProject ? `in the ${currentProject} project` : 'in my system'}.`
          : `I couldn't access my memory system properly${currentProject ? ` for the ${currentProject} project` : ''}.`;
        
        const vaultInfo = contextData.vault?.length > 0
          ? `I also found ${contextData.vault.length} relevant files in the vault.`
          : '';

        content = `I apologize, but I'm having trouble connecting to the model service.

Error: ${fetchError.message}

${memoryInfo} ${vaultInfo}

${contextData.memory.length > 0 ? '\nFrom my memory, I can see we\'ve discussed:\n' + 
  contextData.memory.slice(0, 3).map(m => {
    const summary = m.summary || m.content?.slice(0, 100) || 'Previous conversation';
    return `• ${summary}`;
  }).join('\n') : ''}

Please check:
1. Ollama is running (for local models)
2. API keys are configured (for cloud models)
3. Your internet connection is stable`;
      }

      // === SAVE TO MEMORY WITH PROJECT CONTEXT (even if inference failed) ===
      if (global.memorySystem?.processConversation) {
        try {
          await global.memorySystem.processConversation(message, content, {
            model,
            project: currentProject,
            topic: 'general',
            source: 'chat:send',
            timestamp: new Date().toISOString(),
            hadContext: contextData.memory.length > 0,
            hadVaultResults: contextData.vault?.length > 0,
            inferenceError: inferenceError?.message
          });
          console.log('[MEMORY] Conversation saved to memory capsule with project:', currentProject || 'global');
        } catch (err) {
          console.error('[MEMORY] Failed to save conversation:', err.message);
        }
      }

      // === RETURN RESPONSE ===
      return { 
        content, 
        model,
        project: currentProject || 'global',
        debug: {
          identityUsed: currentIdentity.ai.name,
          memoriesInjected: contextData.memory?.length || 0,
          vaultResultsInjected: contextData.vault?.length || 0,
          contextLength: contextData.context?.length || 0,
          memorySystemInitialized: !!global.memorySystem?.vaultManager?.index,
          projectContext: currentProject || 'global'
        }
      };
      
    } catch (err) {
      console.error('[CHAT:SEND] Fatal error:', err);
      return {
        content: `Error: ${err.message}\n\nPlease check that your memory system and model service are running properly.`,
        model: context?.model || 'unknown',
        error: true
      };
    }
  };
}

module.exports = { createChatSendHandler };